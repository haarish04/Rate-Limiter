A simple txt file to keep track of my progress and how I designed this application

Initially tried to setup a web server using elysia.js just as a way of experimenting new tools but faced some issues when I tried setting it up and hence decided to go for the classic node + express
A simple express app was hosted on port 3000 and some basic routes were setup.

The next step was to setup postman to test the API endpoints were working or not.
Setup postman extension on vscode for testing. Created new collection called testing to test all the routes. With this the basic web server has been hosted with basic routes along with Postman testing setup.
Running performance tests is not available in the extension, so had to boot up the Postman client for it.

Setup all the endpoints in one collection and run performance test by setting up the number of virtual users and time duration. To stress test the API, ramp up the virtual users.
Rther then using type: module in the package.json file, if I leave it as default, it assumes the .mjs/.cjs extension. In order to import packages I need to use the require statement instead of import.

Now moving onto the rate limiting algorithm, there are a number of algorithms to pick from. I will be trying out a few of them such as:

Token Bucket: Tokens are added to a ‘bucket’ at a fixed rate. The bucket has a fixed capacity. When a request is made it will only be accepted if there are enough tokens in the bucket. Tokens are removed from the bucket when a request is accepted.
Fixed window counter - Record the number of requests from a sender occurring in the rate limit’s fixed time interval, if it exceeds the limit the request is rejected.
Sliding window counter - Similar to the fixed window, but each request is timestamped and the window slides.

<---------------------------------------------------------------->

The first implementation is using token bucket algo. 

There is a ‘bucket’ that has capacity for N tokens. Usually this is a bucket per user or IP address.
Every time period a new token is added to the bucket, if the bucket is full the token is discarded.
When a request arrives and the bucket contains tokens, the request is handled and a token is removed from the bucket.
When a request arrives and the bucket is empty, the request is declined.

In the first part of the implementation, we are not dealing with IP addresses and just using the virtual users as individual users during testing in postman

First round of testing: Performance testing with ramp up virtual users upto 50 for 5 mins. Error rate was 0, the problem being, when there is excess load, I am not throttling requests and instead sending back response with "Overload".
This is not considered as error since a response is generated. In order to truly test the throttling, I need to send response code of error
Fixed the code to call the function appropriately and also to generate error status code. This was tested: spike pattern to go upto 60 users for 10 secs out of 2 mins of testing
Results visible in Token Bucket testing figure

Another thing to keep in mind is that postman lets us configure the number of users not specifically no. of requests per second, considering a response takes on average 3-4 ms. Per second there could be upto 250-333 requests per second. But at around 12 -13 requests per second, you can observe requests being failed.
 
<---------------------------------------------------------------->

The next implementation is using fixed window counter algorithm

A window size of N seconds is used to track the request rate. Each incoming request increments the counter for the window.
If the counter exceeds a threshold, the request is discarded.
The windows are typically defined by the floor of the current timestamp, so 17:47:13 with a 60 second window length, would be in the 17:47:00 window.

To implement this, I used date.now() at every request and floor it to the nearest second. My implementation is handling x requests per second so I keep track of the nearest second.
For every request I also keep track of exact time and see if it falls under which second (which window) if count is equal to limit for that window, throttle the request, else accept it.
If request minus window time is grater than 1 second, it falls under new window, refresh counter and start new window.

So the problem with fixed window is, we scam the window by sending a short burst of requests near the boundary of each window. Suppose a rate limiter implementation of 10 requests per 5 secs.
in the first 2 secs we have 3 requests
In the next 3 secs we have 7 requests
After this the next window starts and so we can have,
In the next 2 secs we have 8 requests
Last 3 secs 2 requests.

According to the algorithm this is allowed but if we notice, in the time period of 3 secs + 2 secs in the intersection of the windows, we have 15 requests which should fail.
To tackle this we need sliding window

<---------------------------------------------------------------->

The next algorithm is the sliding window method.

Like the fixed window algorithm, we maintain a counter for each fixed window. But we will need to store the current and the previous windows counts.
We use a weighted count of the current and previous windows counts to determine the count for the sliding window. This helps smooth out the impact of burst of traffic. For example, if the current window is 40% through, then we weight the previous window’s count by 60% and add that to the current window count.
As this algorithm stores relatively little data it suitable for scale, including scaling across distributed clusters.

The advantage of this algorithm is that it does not suffer from the boundary conditions of fixed windows. The limit will be enforced precisely and because the sliding log is tracked for each consumer, you don’t have the issue that every use can suddenly surge in requests each time a fixed window boundary passes.

As discussed in fixed window, the problem exists as there is a burst of requests around the boundary times. To tacke this taking the same example as before:

in the first 2 secs we have 3 requests
In the next 3 secs we have 7 requests
After this the next window starts and so we can have,
In the next 2 secs we have 8 requests
Last 3 secs 2 requests.

When the second window starts, at 2 secs we get 8 requests so we consider not only the current window, but we look at the 5 secs around this period.
We observe how many requests were accepted in the previous 3 secs (windowsize 5 - 2 in next window). Previous window has 10 requests in 5 secs so that is on average 6 requests in previous 3 secs.
Now in new window start 2 secs when 8 requests appear only are accepted

<---------------------------------------------------------------->

After implementing these 3 algorithms the testing images can be found. Fixed window appears to show better request acceptance rate as compared to sliding window as it does not address the problem of high burst of requests around the boundary of window.
If you have a hard limit on number of requests the API endpoint can handle, the sliding window is better as it maintains uniformity in accepting requests.
Fixed window may put extra load on the endpoint and lead to undesirable effects 

<---------------------------------------------------------------->

The next step is to extend the sliding window counter rate limiting to work across multiple servers.
A database like Redis to store the count for the requests and have each server update the database so the rate limiting is shared across two or more servers.

To test this run two (or more) servers each on a different port then verify that you can make the full allowance of request between the servers. 
Example: limit the client to 60 requests per minute, send all 60 to one server in a couple of seconds with Postman and then use curl to hit the other server and verify the request is rejected.